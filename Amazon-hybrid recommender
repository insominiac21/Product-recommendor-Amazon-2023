{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:01:09.470691Z","iopub.execute_input":"2025-09-15T15:01:09.470997Z","iopub.status.idle":"2025-09-15T15:01:09.742841Z","shell.execute_reply.started":"2025-09-15T15:01:09.470973Z","shell.execute_reply":"2025-09-15T15:01:09.742101Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Advanced Hybrid Personalized Recommender System\nThis notebook builds a production-quality hybrid recommender:\n- Collaborative Filtering (implicit ALS)\n- Semantic retrieval (SentenceTransformers + FAISS)\n- Normalized hybrid scoring (CF + semantic)\n- Alpha sweep and per-user detailed outputs\n\n**Warning:** heavy model & large sample sizes require substantial RAM/GPU.\n","metadata":{}},{"cell_type":"code","source":"# Install packages (run once)\n!pip install -q datasets implicit sentence-transformers faiss-cpu mlflow\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:01:09.744046Z","iopub.execute_input":"2025-09-15T15:01:09.744700Z","iopub.status.idle":"2025-09-15T15:02:34.761349Z","shell.execute_reply.started":"2025-09-15T15:01:09.744670Z","shell.execute_reply":"2025-09-15T15:02:34.760286Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.9/705.9 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport random\nimport math\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom scipy.sparse import coo_matrix\nimport torch\nimport faiss\nfrom sentence_transformers import SentenceTransformer, util as st_util\nfrom implicit.als import AlternatingLeastSquares\n\n# CONFIG\nSAMPLE_ROWS = 1_000_000   # change based on memory; 1e6 as requested\nSAMPLE_TRAIN_N = 50_000   # sample size for ALS training (keeps ALS fast)\nSAMPLE_EVAL_N = 1000\nMIN_USER_INTERACTIONS = 2\nALS_FACTORS = 128\nALS_ITERS = 25\nTOP_K = 10\nEMB_BATCH = 128\nEMB_CACHE = \"/kaggle/working/product_embeddings_full.pt\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Device:\", DEVICE)\nprint(\"Be careful: SAMPLE_ROWS =\", SAMPLE_ROWS, \"SAMPLE_TRAIN_N =\", SAMPLE_TRAIN_N)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:02:34.762634Z","iopub.execute_input":"2025-09-15T15:02:34.763336Z","iopub.status.idle":"2025-09-15T15:03:00.101054Z","shell.execute_reply.started":"2025-09-15T15:02:34.763292Z","shell.execute_reply":"2025-09-15T15:03:00.100224Z"}},"outputs":[{"name":"stderr","text":"2025-09-15 15:02:46.199561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757948566.385361      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757948566.439573      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\nBe careful: SAMPLE_ROWS = 1000000 SAMPLE_TRAIN_N = 50000\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\n\nprint(\"Loading dataset (streaming) — this may take time...\")\nds = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Electronics\", split=\"full\", streaming=True, trust_remote_code=True)\nsample_iter = ds.take(SAMPLE_ROWS)\nreviews = pd.DataFrame(sample_iter)\nprint(\"Loaded rows (may be less if dataset smaller):\", len(reviews))\nreviews.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:03:00.102759Z","iopub.execute_input":"2025-09-15T15:03:00.103344Z","iopub.status.idle":"2025-09-15T15:03:44.687068Z","shell.execute_reply.started":"2025-09-15T15:03:00.103322Z","shell.execute_reply":"2025-09-15T15:03:44.686357Z"}},"outputs":[{"name":"stdout","text":"Loading dataset (streaming) — this may take time...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f9c437a38049f298f47f59f1db52b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Amazon-Reviews-2023.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8580a2df68b640efbc8e0c316536a951"}},"metadata":{}},{"name":"stdout","text":"Loaded rows (may be less if dataset smaller): 1000000\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   rating                                        title  \\\n0     3.0            Smells like gasoline! Going back!   \n1     1.0      Didn’t work at all lenses loose/broken.   \n2     5.0                                   Excellent!   \n3     5.0                       Great laptop backpack!   \n4     5.0  Best Headphones in the Fifties price range!   \n\n                                                text  \\\n0  First & most offensive: they reek of gasoline ...   \n1  These didn’t work. Idk if they were damaged in...   \n2  I love these. They even come with a carry case...   \n3  I was searching for a sturdy backpack for scho...   \n4  I've bought these headphones three times becau...   \n\n                                              images        asin parent_asin  \\\n0  [{'small_image_url': 'https://m.media-amazon.c...  B083NRGZMM  B083NRGZMM   \n1                                                 []  B07N69T6TM  B07N69T6TM   \n2                                                 []  B01G8JO5F2  B01G8JO5F2   \n3                                                 []  B001OC5JKY  B001OC5JKY   \n4                                                 []  B013J7WUGC  B07CJYMRWM   \n\n                        user_id      timestamp  helpful_vote  \\\n0  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1658185117948             0   \n1  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1592678549731             0   \n2  AFKZENTNBQ7A7V7UXW5JJI6UGRYQ  1523093017534             0   \n3  AGGZ357AO26RQZVRLGU4D4N52DZQ  1290278495000            18   \n4  AG2L7H23R5LLKDKLBEF2Q3L2MVDA  1676601581238             0   \n\n   verified_purchase  \n0               True  \n1               True  \n2               True  \n3               True  \n4               True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>title</th>\n      <th>text</th>\n      <th>images</th>\n      <th>asin</th>\n      <th>parent_asin</th>\n      <th>user_id</th>\n      <th>timestamp</th>\n      <th>helpful_vote</th>\n      <th>verified_purchase</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.0</td>\n      <td>Smells like gasoline! Going back!</td>\n      <td>First &amp; most offensive: they reek of gasoline ...</td>\n      <td>[{'small_image_url': 'https://m.media-amazon.c...</td>\n      <td>B083NRGZMM</td>\n      <td>B083NRGZMM</td>\n      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n      <td>1658185117948</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>Didn’t work at all lenses loose/broken.</td>\n      <td>These didn’t work. Idk if they were damaged in...</td>\n      <td>[]</td>\n      <td>B07N69T6TM</td>\n      <td>B07N69T6TM</td>\n      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n      <td>1592678549731</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.0</td>\n      <td>Excellent!</td>\n      <td>I love these. They even come with a carry case...</td>\n      <td>[]</td>\n      <td>B01G8JO5F2</td>\n      <td>B01G8JO5F2</td>\n      <td>AFKZENTNBQ7A7V7UXW5JJI6UGRYQ</td>\n      <td>1523093017534</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.0</td>\n      <td>Great laptop backpack!</td>\n      <td>I was searching for a sturdy backpack for scho...</td>\n      <td>[]</td>\n      <td>B001OC5JKY</td>\n      <td>B001OC5JKY</td>\n      <td>AGGZ357AO26RQZVRLGU4D4N52DZQ</td>\n      <td>1290278495000</td>\n      <td>18</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>Best Headphones in the Fifties price range!</td>\n      <td>I've bought these headphones three times becau...</td>\n      <td>[]</td>\n      <td>B013J7WUGC</td>\n      <td>B07CJYMRWM</td>\n      <td>AG2L7H23R5LLKDKLBEF2Q3L2MVDA</td>\n      <td>1676601581238</td>\n      <td>0</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Standardize column names (dataset variations)\nrename_map = {\"user_id\":\"reviewerID\",\"asin\":\"asin\",\"rating\":\"overall\",\"text\":\"reviewText\",\"timestamp\":\"unixReviewTime\",\"title\":\"title\"}\nfor k,v in rename_map.items():\n    if k in reviews.columns and v not in reviews.columns:\n        reviews.rename(columns={k:v}, inplace=True)\n\nkeep_cols = [c for c in [\"reviewerID\",\"asin\",\"overall\",\"reviewText\",\"unixReviewTime\",\"title\"] if c in reviews.columns]\nreviews = reviews[keep_cols].copy()\nreviews.dropna(subset=[\"reviewerID\",\"asin\",\"overall\"], inplace=True)\nreviews.drop_duplicates(subset=[\"reviewerID\",\"asin\"], inplace=True)\nprint(\"After preprocessing:\", reviews.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:03:44.687882Z","iopub.execute_input":"2025-09-15T15:03:44.688188Z","iopub.status.idle":"2025-09-15T15:03:46.102217Z","shell.execute_reply.started":"2025-09-15T15:03:44.688167Z","shell.execute_reply":"2025-09-15T15:03:46.101484Z"}},"outputs":[{"name":"stdout","text":"After preprocessing: (997501, 6)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"if \"unixReviewTime\" in reviews.columns:\n    reviews[\"ts\"] = pd.to_datetime(reviews[\"unixReviewTime\"], unit='ms', errors='coerce')\nelse:\n    reviews[\"ts\"] = pd.NaT\n\nreviews.sort_values([\"reviewerID\",\"ts\"], inplace=True)\nuser_counts = reviews[\"reviewerID\"].value_counts()\nvalid_users = user_counts[user_counts >= MIN_USER_INTERACTIONS].index\nreviews = reviews[reviews[\"reviewerID\"].isin(valid_users)].copy()\n\ntest_idx = reviews.groupby(\"reviewerID\").tail(1).index\ntest_df = reviews.loc[test_idx].copy()\ntrain_df = reviews.drop(test_idx).copy()\n\nprint(\"Train size:\", len(train_df), \"Test size:\", len(test_df), \"Unique items in train:\", train_df['asin'].nunique())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:03:46.102987Z","iopub.execute_input":"2025-09-15T15:03:46.103219Z","iopub.status.idle":"2025-09-15T15:03:50.049365Z","shell.execute_reply.started":"2025-09-15T15:03:46.103201Z","shell.execute_reply":"2025-09-15T15:03:50.048555Z"}},"outputs":[{"name":"stdout","text":"Train size: 812259 Test size: 126792 Unique items in train: 277006\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Build the full item_map from train_df (ensures coverage of test ASINs)\nitem_ids = train_df[\"asin\"].unique().tolist()\nprint(\"Total unique items (train_df):\", len(item_ids))\nitem_map = {asin: idx for idx, asin in enumerate(item_ids)}\nindex_to_asin = {i:asin for asin,i in item_map.items()}\n\n# Build ordered_asins and ordered_texts\nordered_asins = [None] * len(item_map)\nfor asin, idx in item_map.items():\n    ordered_asins[idx] = asin\n\n# Aggregate text per product (up to N reviews to keep length moderate)\ngrouped_text_full = train_df.groupby(\"asin\")[\"reviewText\"].apply(lambda s: \" \".join(s.astype(str).values[:10])).to_dict()\nordered_texts = [ grouped_text_full.get(a, \"\") for a in ordered_asins ]\nprint(\"Prepared ordered_texts for\", len(ordered_texts), \"items\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:03:50.050205Z","iopub.execute_input":"2025-09-15T15:03:50.050430Z","iopub.status.idle":"2025-09-15T15:04:05.953128Z","shell.execute_reply.started":"2025-09-15T15:03:50.050403Z","shell.execute_reply":"2025-09-15T15:04:05.952417Z"}},"outputs":[{"name":"stdout","text":"Total unique items (train_df): 277006\nPrepared ordered_texts for 277006 items\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Sample interactions for ALS training (faster)\ntrain_sample = train_df.sample(min(SAMPLE_TRAIN_N, len(train_df)), random_state=42).copy()\n\n# ALS user map is from the sampled users (only these users will have ALS factors)\nals_user_ids = train_sample[\"reviewerID\"].unique().tolist()\nals_user_map = {u:i for i,u in enumerate(als_user_ids)}\nals_index_to_user = {i:u for u,i in als_user_map.items()}\n\n# Map sample rows to indices in sample-user map and full item_map\nrows = train_sample[\"reviewerID\"].map(als_user_map).astype(int)\ncols = train_sample[\"asin\"].map(item_map).astype(int)   # map into full item space\nvals = train_sample[\"overall\"].astype(float)\n\ninteraction_matrix = coo_matrix((vals, (rows, cols)), shape=(len(als_user_ids), len(item_map)))\nmatrix_csr = interaction_matrix.tocsr()\nprint(\"ALS training matrix shape:\", matrix_csr.shape, \"nnz:\", interaction_matrix.nnz)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:04:05.953884Z","iopub.execute_input":"2025-09-15T15:04:05.954146Z","iopub.status.idle":"2025-09-15T15:04:06.400521Z","shell.execute_reply.started":"2025-09-15T15:04:05.954128Z","shell.execute_reply":"2025-09-15T15:04:06.399840Z"}},"outputs":[{"name":"stdout","text":"ALS training matrix shape: (32295, 277006) nnz: 50000\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"\"\"\"\n# Train ALS on item-user (implicit expects items x users for fit)\nals = AlternatingLeastSquares(factors=ALS_FACTORS, regularization=0.1, iterations=ALS_ITERS, calculate_training_loss=False)\nitem_user = matrix_csr.T.tocsr()\nprint(\"Training ALS ... (this may take a while)\")\nals.fit(item_user)\nprint(\"ALS trained; user_factors shape:\", als.user_factors.shape, \" item_factors shape:\", als.item_factors.shape)\n# Quick sanity checks:\nassert als.user_factors.shape[0] == item_user.shape[1]  # number of users in ALS (should equal len(als_user_ids))\nassert als.item_factors.shape[0] == item_user.shape[0]  # number of items in item space\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:04:06.401262Z","iopub.execute_input":"2025-09-15T15:04:06.401471Z","iopub.status.idle":"2025-09-15T15:04:06.406437Z","shell.execute_reply.started":"2025-09-15T15:04:06.401453Z","shell.execute_reply":"2025-09-15T15:04:06.405713Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\n# Train ALS on item-user (implicit expects items x users for fit)\\nals = AlternatingLeastSquares(factors=ALS_FACTORS, regularization=0.1, iterations=ALS_ITERS, calculate_training_loss=False)\\nitem_user = matrix_csr.T.tocsr()\\nprint(\"Training ALS ... (this may take a while)\")\\nals.fit(item_user)\\nprint(\"ALS trained; user_factors shape:\", als.user_factors.shape, \" item_factors shape:\", als.item_factors.shape)\\n# Quick sanity checks:\\nassert als.user_factors.shape[0] == item_user.shape[1]  # number of users in ALS (should equal len(als_user_ids))\\nassert als.item_factors.shape[0] == item_user.shape[0]  # number of items in item space\\n'"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Cell A: diagnose + try both orientations and pick the one whose shapes match\nimport time\nfrom implicit.als import AlternatingLeastSquares\n\nn_als_users = len(als_user_ids)            # number of users in ALS sample\nn_items_full = len(item_map)               # total items (full item_map)\nprint(\"Expected ALS users:\", n_als_users, \"Expected items:\", n_items_full)\nprint(\"matrix_csr.shape (sample-user x full-items):\", matrix_csr.shape)\nprint(\"matrix_csr.T.shape (full-items x sample-users):\", matrix_csr.T.shape)\n\ndef try_fit(mat, factors=ALS_FACTORS, iters=ALS_ITERS, reg=0.1):\n    t0 = time.time()\n    model = AlternatingLeastSquares(factors=factors, regularization=reg, iterations=iters, calculate_training_loss=False)\n    model.fit(mat)\n    t = time.time()-t0\n    return model, t\n\n# --- Try option 1: fit on item_user (items x users) (this is typical) ---\nprint(\"\\nTrying fit on item_user (items x users) ...\")\ntry:\n    model1, t1 = try_fit(matrix_csr.T.tocsr())\n    print(\"Took %.1f s. model1.user_factors.shape=%s model1.item_factors.shape=%s\" % (t1, model1.user_factors.shape, model1.item_factors.shape))\nexcept Exception as e:\n    model1 = None\n    print(\"Error fitting model1:\", e)\n\nok1 = (model1 is not None) and (model1.user_factors.shape[0] == n_als_users) and (model1.item_factors.shape[0] == n_items_full)\n\n# --- Try option 2: fit on users x items (less common for implicit, but check) ---\nprint(\"\\nTrying fit on matrix_csr (users x items) ...\")\ntry:\n    model2, t2 = try_fit(matrix_csr.tocsr())\n    print(\"Took %.1f s. model2.user_factors.shape=%s model2.item_factors.shape=%s\" % (t2, model2.user_factors.shape, model2.item_factors.shape))\nexcept Exception as e:\n    model2 = None\n    print(\"Error fitting model2:\", e)\n\nok2 = (model2 is not None) and (model2.user_factors.shape[0] == n_als_users) and (model2.item_factors.shape[0] == n_items_full)\n\n# Decide\nif ok1 and not ok2:\n    als = model1\n    ALS_FIT_ORIENTATION = \"items_x_users\"   # we trained on item_user\n    print(\"\\nSelected model1 (items x users). ALS aligned to ALS users and full items.\")\nelif ok2 and not ok1:\n    als = model2\n    ALS_FIT_ORIENTATION = \"users_x_items\"\n    print(\"\\nSelected model2 (users x items). ALS aligned to ALS users and full items.\")\nelif ok1 and ok2:\n    # both worked — prefer item_user (implicit convention)\n    als = model1\n    ALS_FIT_ORIENTATION = \"items_x_users_both_ok\"\n    print(\"\\nBoth fits produced matching shapes; chose model1 (items x users).\")\nelse:\n    # Neither fit matched exactly — print diagnostics and keep the model that is 'closer'\n    print(\"\\nNeither training orientation produced perfect alignment. Printing diagnostics...\")\n    print(\"model1 shapes:\", None if model1 is None else (model1.user_factors.shape, model1.item_factors.shape))\n    print(\"model2 shapes:\", None if model2 is None else (model2.user_factors.shape, model2.item_factors.shape))\n    # Choose the one where user_factors match n_als_users if any\n    if model1 is not None and model1.user_factors.shape[0] == n_als_users:\n        als = model1; ALS_FIT_ORIENTATION = \"items_x_users_partial\"\n        print(\"Chose model1 because user_factors matched number of ALS users.\")\n    elif model2 is not None and model2.user_factors.shape[0] == n_als_users:\n        als = model2; ALS_FIT_ORIENTATION = \"users_x_items_partial\"\n        print(\"Chose model2 because user_factors matched number of ALS users.\")\n    else:\n        # fallback: choose the faster one\n        candidate = (model1, t1) if (model1 is not None and (model2 is None or t1 <= t2)) else (model2, t2)\n        als = candidate[0]\n        ALS_FIT_ORIENTATION = \"fallback_time_choice\"\n        print(\"Fallback: chose the faster fitted model (inspect manually).\")\n\nprint(\"\\nFinal ALS shapes: user_factors:\", als.user_factors.shape, \" item_factors:\", als.item_factors.shape)\nprint(\"ALS_FIT_ORIENTATION set to:\", ALS_FIT_ORIENTATION)\n\n# Make sure als and als_user_map variables are available downstream\n# (we trained ALS on sample users, so the mapping to use when calling recommend is als_user_map)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:04:06.408672Z","iopub.execute_input":"2025-09-15T15:04:06.409097Z","iopub.status.idle":"2025-09-15T15:04:24.969273Z","shell.execute_reply.started":"2025-09-15T15:04:06.409079Z","shell.execute_reply":"2025-09-15T15:04:24.968469Z"}},"outputs":[{"name":"stdout","text":"Expected ALS users: 32295 Expected items: 277006\nmatrix_csr.shape (sample-user x full-items): (32295, 277006)\nmatrix_csr.T.shape (full-items x sample-users): (277006, 32295)\n\nTrying fit on item_user (items x users) ...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/implicit/cpu/als.py:95: RuntimeWarning: Intel MKL BLAS is configured to use 2 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'MKL_NUM_THREADS=1' or by callng 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having MKL use a threadpool can lead to severe performance issues\n  check_blas_config()\n/usr/local/lib/python3.11/dist-packages/implicit/cpu/als.py:95: RuntimeWarning: OpenBLAS is configured to use 4 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n  check_blas_config()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e5660d2eddd404b9141e83692fc301e"}},"metadata":{}},{"name":"stdout","text":"Took 9.5 s. model1.user_factors.shape=(277006, 128) model1.item_factors.shape=(32295, 128)\n\nTrying fit on matrix_csr (users x items) ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/25 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3b70e3eda54757922129e922ae2ff9"}},"metadata":{}},{"name":"stdout","text":"Took 9.0 s. model2.user_factors.shape=(32295, 128) model2.item_factors.shape=(277006, 128)\n\nSelected model2 (users x items). ALS aligned to ALS users and full items.\n\nFinal ALS shapes: user_factors: (32295, 128)  item_factors: (277006, 128)\nALS_FIT_ORIENTATION set to: users_x_items\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell B: sanity test - pick a sample ALS user and request recommendations\n# Choose a sample user that was in the ALS sample\nsample_als_users = list(als_user_ids)[:5]   # these are user ids from the sample used for ALS\nprint(\"Sample ALS user ids (from ALS sample):\", sample_als_users)\n\n# pick first user and compute recommendation via als.recommend using the als_user_map index\ntest_uid = sample_als_users[0]\nif test_uid in als_user_map:\n    uidx = als_user_map[test_uid]\n    try:\n        # recommend returns item indices (internal indices) and scores\n        items_idx, scores = als.recommend(uidx, matrix_csr[uidx], N=10) \n        items_idx = [int(x) for x in items_idx]\n        print(\"ALS recommended internal item indices:\", items_idx)\n        # Map internal indices to ASINs using index_to_asin\n        rec_asins = [index_to_asin.get(idx, \"<UNK>\") for idx in items_idx]\n        print(\"ALS recommended ASINs:\", rec_asins)\n    except Exception as e:\n        print(\"Error calling als.recommend:\", e)\nelse:\n    print(\"Sample test user not found in als_user_map - can't test ALS recommendation for them.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:04:24.970168Z","iopub.execute_input":"2025-09-15T15:04:24.970533Z","iopub.status.idle":"2025-09-15T15:04:24.997169Z","shell.execute_reply.started":"2025-09-15T15:04:24.970508Z","shell.execute_reply":"2025-09-15T15:04:24.996377Z"}},"outputs":[{"name":"stdout","text":"Sample ALS user ids (from ALS sample): ['AGVKCYW5RVRP2CELCHPDFTHZZ6AA', 'AEDSLRR3V26BMGRPKB3EWN3CZN4Q', 'AH63Q56CTP2KRUCZM4YWZI5QHF4A', 'AEAS6A3ESOYYXPU2SVGDY7JNJ6QA', 'AGNRZ3IMB3FIQB72KEGOYRYL4ZVA']\nALS recommended internal item indices: [29, 8328, 221, 6398, 4356, 20, 370, 7170, 4981, 575]\nALS recommended ASINs: ['B00ZV9RDKK', 'B0792K2BK6', 'B004OVECU0', 'B007OZNZG0', 'B001F42MKG', 'B005DOK8NW', 'B01DM6BDA4', 'B00DIFIP06', 'B004WYA852', 'B079QHML21']\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Choose a strong SentenceTransformers model\n#model_name = \"all-mpnet-base-v2\"   # good balance of accuracy and memory\nmodel_name = \"all-roberta-large-v1\"  # alternative if you have lots of memory and want max quality\n\nprint(\"Loading embedding model:\", model_name)\nmodel = SentenceTransformer(model_name, device=DEVICE)\n\n# Encode (or load cached)\nif os.path.exists(EMB_CACHE):\n    print(\"Loading cached embeddings from\", EMB_CACHE)\n    embeddings = torch.load(EMB_CACHE).cpu().numpy()\nelse:\n    print(\"Encoding embeddings for full item set... (this will take a long time for large item counts)\")\n    all_embs = []\n    for i in range(0, len(ordered_texts), EMB_BATCH):\n        batch = ordered_texts[i:i+EMB_BATCH]\n        emb = model.encode(batch, convert_to_tensor=True, show_progress_bar=True)\n        all_embs.append(emb.cpu())\n    embeddings = torch.cat(all_embs, dim=0).numpy().astype('float32')\n    torch.save(torch.tensor(embeddings), EMB_CACHE)\nprint(\"Embeddings shape:\", embeddings.shape)\nassert embeddings.shape[0] == len(item_map)\n\n# Normalize embeddings and build FAISS index for inner-product (cosine)\nfaiss.normalize_L2(embeddings)\nemb_dim = embeddings.shape[1]\nindex = faiss.IndexFlatIP(emb_dim)\nindex.add(embeddings)\nprint(\"FAISS index built. Total items in index:\", index.ntotal)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:04:24.997995Z","iopub.execute_input":"2025-09-15T15:04:24.998232Z"}},"outputs":[{"name":"stdout","text":"Loading embedding model: all-roberta-large-v1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d8ba543f0c4ec7b90c54b892af0ad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0661944eabd84eea9c0b3ada6fcdd59b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2a50b1b4cf48ba8f822dab0c55aac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59cc403cd726479abd3e6a9d17e5c891"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"730a797209a24280a1bb08147cc7717f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e179d98bd94c21b5d1f9290732208b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/328 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46460e513b5c4016809d54841d13f5fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288356c4dd68443aacbaee798f60da99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb37c6ef045044598619f4bdb3dca622"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55dfbfa7b30f4b4fb8048eb1db24de59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf236d0ed0be469d82cc58c48eb2bcdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c14c1ab19ee42e082441138bf2183d4"}},"metadata":{}},{"name":"stdout","text":"Encoding embeddings for full item set... (this will take a long time for large item counts)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27f36ff8c4f49eeb331f9daaea5788d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52175eb3f9ee46409a7b2a601b75e8c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a123380e2c5448a3918434cfeee7d8dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ae0b184e9a423694e01d1d2cf4ff21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55dfa37be20a41c8b9b82dfb8f85ac72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b09ca0e041cb4410962e08981b8bb075"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68753338ffc24fbd8f4a7b4e9bd7662d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"010161e71260424ea674c4c776a4b445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52598c0df23d470c9f4281912d23e02b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b81bbab9476452a9177b7459eeb6a2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233efae7292a490da66fbe152ce97349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eb1ec3fcd034926a8cbc6b7fabed3dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5490e8639378439aa2f6c998fcb57688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bcbb38a7d4245529e2549f3f2b27277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e6c7c6d78d84d2f99c046f9f75f2d4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6bf5ce5b841417d80b5dd607d36885e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d736a03a72834a11ad113a90b28cb478"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"977f651e5625423e9e81c7f5d88dc391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9499b221c9614a9d90dd0f47ab5dc2c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fbeff96ea3245e2b95d404f58c0ae68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b115ceff24f3486ca4efadb6fbc6463a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4e07b43172346ecadb49c8418fd061c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9e333b3e55b45c58c19b475dba7d68c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"916348fd974b4ec7befde8b259af73dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b48a9cd4bb54a19809b31471bc99560"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9edc6aad4ac41d99c6c3652afde934a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9decaf5ceac04f149336ca6be86b0dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6b7910f95264655a6a841634ab9f924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0186ac9685984e30931d151c91af0c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aea1892a5ad34117931eabf28b563aaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a31c15e0f2df4d06b7ca2cb76fcdc2e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2203753069644531bd220ccee831c92e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78f10e299db040d8860b81ee56c13d88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2ca4e98846547ecb8556036ba418ede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06dddb52396d4cd6a6888da3f775a09b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"314e8f6b69814387a07cdca7c9df9c33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"604dfc00971940da924694232020294b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d92de12d50cc46f69f2bf19cfacc6bff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222aa1757e4a4e26bebf7416480befc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4551fd85747d4b3e8174758b27b3321e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc11d4e02bfc4d00ade7a9bee872cbf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"715d4ddc228d431e96c14b62da6db494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"741ef99e46c5498b8000048006f5e555"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55bfa9660a4040b9bf21eb4ed67505ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae4f600cfea443a9b9283218d4b26b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ac423f9b4f84661b3af13ca126aa5de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8afba3841dc04164b63f7b7397af78e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a67845398641d5b484e933d58e63e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"242215cd79c84e598fdf421f2583eed1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0ce7e0553d44491812d21250926f25d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad8ab45cb2d439b90f0c278296b3e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ef0269b9444442f8154133cabca7f6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e66d802c4d86442c939bab991b40387e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd667045b2ac4b0e861804231494693d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21085040f887409088f2bf01ed0ac4ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31cbdbe7b47a4a8a8f404cdab72a243b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcecc1df11fe44689cfe41faf0ba9664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87708b232ebb48e7b26d53ef34931daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14fbff216e14ff8ad22903dce308b3d"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Build rich user profiles from the full train_df (history for each user)\nuser_profile = {}  # user_id -> numpy (1, dim)\nfor uid, group in train_df.groupby(\"reviewerID\"):\n    asins = [a for a in group[\"asin\"].unique().tolist() if a in item_map]\n    if not asins:\n        continue\n    idxs = [item_map[a] for a in asins]\n    emb_stack = embeddings[idxs]  # numpy\n    user_profile[uid] = emb_stack.mean(axis=0, keepdims=True).astype('float32')\n\nprint(\"Computed user profiles for\", len(user_profile), \"users\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hybrid_recommend(user_id, topn=10, alpha=0.7, cf_N=50, sem_N=100):\n    \"\"\"\n    Returns list of tuples: (asin, final_score, cf_norm, sem_norm)\n    - alpha: weight for CF (0..1)\n    - cf_N: number of CF candidates to consider\n    - sem_N: number of semantic (FAISS) candidates to request\n    \"\"\"\n    # Popular fallback\n    popular_items = [p for p,_ in Counter(train_df[\"asin\"]).most_common(topn)]\n    if user_id not in user_profile:\n        return [(p, 0.0, 0.0, 0.0) for p in popular_items]\n\n    # CF candidates (only if user in ALS sampled users)\n    cf_candidates = []\n    cf_scores = {}\n    if user_id in als_user_map:\n        uidx = als_user_map[user_id]\n        try:\n            item_indices, scores = als.recommend(uidx, matrix_csr[uidx], N=cf_N)\n            cf_candidates = [int(x) for x in item_indices]\n            cf_scores = {int(idx): float(s) for idx,s in zip(item_indices, scores)}\n        except Exception as e:\n            cf_candidates = []\n            cf_scores = {}\n\n    # Semantic (FAISS) candidates\n    query = user_profile[user_id].astype('float32')  # shape (1, dim)\n    faiss.normalize_L2(query)\n    D, I = index.search(query, sem_N)\n    sem_candidates = [int(i) for i in I[0] if i >= 0]\n\n    # Union candidates\n    candidate_set = set(cf_candidates) | set(sem_candidates)\n\n    # Normalize CF scores (scale to [0,1])\n    if cf_scores:\n        max_cf = max(cf_scores.values())\n        min_cf = min(cf_scores.values())\n        # if all cf <=0, we'll shift to 0..1 by subtracting min if needed\n        denom_cf = (max_cf - min_cf) if (max_cf - min_cf) > 0 else (abs(max_cf) + 1e-9)\n    else:\n        max_cf = 0.0\n        min_cf = 0.0\n        denom_cf = 1.0\n\n    hybrid_scores = []\n    # Precompute normalized query (for dot)\n    # Note: embeddings are already L2-normalized for FAISS; query normalized above.\n    query_vec = query\n\n    for idx in candidate_set:\n        # CF normalized\n        cf_raw = cf_scores.get(idx, 0.0)\n        if cf_scores:\n            cf_norm = (cf_raw - min_cf) / denom_cf\n        else:\n            cf_norm = 0.0\n\n        # semantic normalized: cosine in [-1,1] -> map to [0,1]\n        emb_vec = embeddings[idx].reshape(1, -1).astype('float32')\n        # both normalized -> dot product equals cosine\n        sem_score = float(np.dot(query_vec, emb_vec.T)[0][0])\n        sem_norm = (sem_score + 1.0) / 2.0\n\n        final = alpha * cf_norm + (1.0 - alpha) * sem_norm\n        hybrid_scores.append((idx, final, cf_norm, sem_norm))\n\n    # Sort and map to ASINs\n    hybrid_scores.sort(key=lambda x: -x[1])\n    result = []\n    for idx, final, cf_n, sem_n in hybrid_scores[:topn]:\n        asin = index_to_asin.get(idx, None)\n        if asin:\n            result.append((asin, final, cf_n, sem_n))\n    # If empty fallback to popular\n    if not result:\n        return [(p, 0.0, 0.0, 0.0) for p in popular_items]\n    return result\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def precision_at_k(recommended, ground_truth, k):\n    if not recommended:\n        return 0.0\n    return sum([1 for r in recommended[:k] if r in ground_truth]) / k\n\ndef recall_at_k(recommended, ground_truth, k):\n    if not ground_truth:\n        return 0.0\n    return sum([1 for r in recommended[:k] if r in ground_truth]) / len(ground_truth)\n\ndef ndcg_at_k(recommended, ground_truth, k):\n    dcg = sum((1.0 / math.log2(i + 2)) for i, r in enumerate(recommended[:k]) if r in ground_truth)\n    idcg = sum((1.0 / math.log2(i + 2)) for i in range(min(len(ground_truth), k)))\n    return dcg / idcg if idcg > 0 else 0.0\n\ndef hit_rate_at_k(recommended, ground_truth, k):\n    return 1.0 if any(r in ground_truth for r in recommended[:k]) else 0.0\n\ndef average_precision_at_k(recommended, ground_truth, k):\n    if not ground_truth:\n        return 0.0\n    score = 0.0\n    num_hits = 0.0\n    for i, p in enumerate(recommended[:k]):\n        if p in ground_truth:\n            num_hits += 1.0\n            score += num_hits / (i + 1.0)\n    return score / min(len(ground_truth), k)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# quick assertions to avoid silent mapping bugs\nassert embeddings.shape[0] == len(item_map), \"embeddings must match full item_map length\"\nassert matrix_csr.shape[1] == len(item_map), \"ALS interaction matrix columns must match item_map dimension\"\nprint(\"Sanity checks passed.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Build evaluation user list: only users with user_profile and whose test item is in item_map\neval_users = [u for u in test_df[\"reviewerID\"].unique() \n              if (u in user_profile) and (test_df.loc[test_df[\"reviewerID\"]==u, \"asin\"].iloc[0] in item_map)]\nprint(\"Eligible eval users (test item covered & profile built):\", len(eval_users))\n\neval_sample = random.sample(eval_users, min(SAMPLE_EVAL_N, len(eval_users)))\nalpha_values = [0.3, 0.5, 0.7, 0.9]\n\nfor alpha in alpha_values:\n    print(\"\\n=== Evaluating for alpha =\", alpha, \"===\\n\")\n    precisions, recalls, ndcgs, hits, maps = [], [], [], [], []\n\n    for u in eval_sample:\n        recs = hybrid_recommend(u, topn=TOP_K, alpha=alpha, cf_N=50, sem_N=100)\n        recommended_asins = [r[0] for r in recs]\n        true_asin = test_df.loc[test_df[\"reviewerID\"]==u, \"asin\"].iloc[0]\n        true_items = set([true_asin])\n\n        prec = precision_at_k(recommended_asins, true_items, TOP_K)\n        rec = recall_at_k(recommended_asins, true_items, TOP_K)\n        ndcg = ndcg_at_k(recommended_asins, true_items, TOP_K)\n        hit = hit_rate_at_k(recommended_asins, true_items, TOP_K)\n        ap = average_precision_at_k(recommended_asins, true_items, TOP_K)\n\n        precisions.append(prec)\n        recalls.append(rec)\n        ndcgs.append(ndcg)\n        hits.append(hit)\n        maps.append(ap)\n\n    print(f\"Alpha={alpha}: Precision@{TOP_K}={np.mean(precisions):.6f}, Recall@{TOP_K}={np.mean(recalls):.6f}, NDCG@{TOP_K}={np.mean(ndcgs):.6f}, HitRate@{TOP_K}={np.mean(hits):.6f}, MAP@{TOP_K}={np.mean(maps):.6f}\")\n\n# Print detailed recommendations for the first 5 eval users\nprint(\"\\n=== Detailed recommendations (first 5 eval users) ===\")\nfor u in eval_sample[:5]:\n    recs = hybrid_recommend(u, topn=TOP_K, alpha=0.6)\n    print(\"\\nUser:\", u)\n    for asin, final, cf_n, sem_n in recs:\n        print(f\"  {asin}  final={final:.4f}  cf_norm={cf_n:.4f}  sem_norm={sem_n:.4f}\")\n    print(\"  True item:\", test_df.loc[test_df['reviewerID']==u, 'asin'].iloc[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save models / artifacts for reuse\nimport joblib\nartifact_dir = \"/kaggle/working/recommender_artifacts\"\nos.makedirs(artifact_dir, exist_ok=True)\n\n# Save ALS factors (numpy)\nnp.save(os.path.join(artifact_dir, \"als_user_factors.npy\"), als.user_factors)\nnp.save(os.path.join(artifact_dir, \"als_item_factors.npy\"), als.item_factors)\n# Save embeddings\nnp.save(os.path.join(artifact_dir, \"embeddings.npy\"), embeddings)\n# Save maps\njoblib.dump(item_map, os.path.join(artifact_dir, \"item_map.joblib\"))\njoblib.dump(als_user_map, os.path.join(artifact_dir, \"als_user_map.joblib\"))\nprint(\"Artifacts saved to\", artifact_dir)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final notes & next steps\n\n- If metrics are still low:\n  - Increase SAMPLE_TRAIN_N and/or SAMPLE_ROWS (if memory allows).\n  - Try `all-roberta-large-v1` if you have enough GPU memory — it can improve semantic quality.\n  - Increase CF candidate size `cf_N` and semantic `sem_N`, and experiment with alpha.\n  - Consider using session-based or sequence models (SASRec) for better personalization.\n  - Consider negative sampling / more advanced evaluation (multiple held-out items per user).\n\n- For MLOps:\n  - Track experiments with MLflow (parameters: alpha, ALS_FACTORS, ALS_ITERS, model_name).\n  - Version embeddings and ALS factors.\n  - Use FAISS index persistence and incremental updates when adding new items.\n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}